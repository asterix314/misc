#set text(
  font: "FangSong", 
  size: 14pt,
  lang: "zh"
)

#set heading(numbering: "1.1")

#show heading.where(
  level: 1
): it => block(width: 100%)[
  #set align(center)
  #set text(16pt, font: "SimHei")
  #it
]

#show heading: set block(spacing: 1.2em)
#show raw: set text(font: ("Fira Code", "FangSong"))
#show strong: it => text(font: "SimHei", it.body)

#show figure.caption: it => {
  set text(font: "FangSong")
  it
}

#grid(
  columns: (0.8em, auto),
  grid.vline(
    x: 0, 
    end: 2, 
    stroke: 10pt + blue),
  [],
)[五矿集团“数字化企业大脑”项目\
数据治理文档]

#v(30%)

#align(center, text(30pt)[
  *集成开发平台产品操作规范*\
  （草稿）
])

#align(right + bottom,
    datetime.today().display("[year] 年 [month] 月 [day] 日更新"))

#show table.cell.where(y: 0): set text(weight: "bold")

#pagebreak()

#set page(
  header: [#align(right, text(
    size: 12pt, 
    font: "FangSong", 
    baseline: 8pt,
    [集成开发平台产品操作规范]))
    #line(length: 100%)]
)

#outline()

#set text(
  font: ("Times New Roman", "SimSun"), 
  size: 12pt,
  lang: "zh"
)

#set page(
  numbering: "1",
)

#set par(justify: true)

#show heading.where(level: 1): it => {
  pagebreak(weak: true)
  it
}

#let 菜单(..items) = {
  let path = items
    .pos()
    .join(h(.4em) + box(baseline: -2pt, [▶]) + h(.4em))
  set text(font: "FangSong")
  [「] + [#path] + [」]
}

// 段落首行缩进
#show: body => {
  for (ie, elem) in body.children.enumerate() {
    if elem.func() == text or elem.func() == strong {
      if ie > 0 and body.children.at(ie - 1).func() == parbreak {
        h(2em)
      }
      elem
    } else {
      elem
    }
  }
}

#counter(page).update(1)

数据集成与开发旨在帮助企业解决日益复杂的数据采集与利用问题，其核心功能是整合来自不同源头、格式各异的数据资源，通过抽取、转换、加载等一系列过程，最终输出满足业务需要的数据，为上层数据应用赋能。 

该平台主要面向数据开发人员，为其提供可视化的数据开发界面，简化数据加工流程的设计与构建，使得开发团队能够快速响应业务需求变化，实施数据项目，加速数据分析和应用开发的进程。主要涵盖多源异构数据采集、数据流水线编排、数据任务的开发调试与调度、版本控制、运维监控、性能优化等全方位的功能，为企业的数据驱动战略奠定坚实基础。

一些名词解释：

/ 数据任务：: 定义数据同步或数据加工的逻辑，实际执行数据的同步或加工动作。

/ 数据流水线：: 将多个数据同步或加工任务，按执行顺序编排而成的数据加工流程。

/ 实时数据同步：:	当来源数据库发生数据变化时，即时同步数据到目标数据库中。

/ 离线数据同步：:	通常一次性、或按照一定的时间间隔，将批量的数据同步到目标数据库。

/ 任务调度：: 在复杂的数据处理流程中，自动化地组织和管理各个数据处理任务的执行顺序、依赖关系、计算资源，提高数据处理的效率和准确性，减少人工干预和错误。

/ 数据开发：:	数据开发关注从原始数据中获取有价值的信息，通过可视化配置或代码编程等方式，创建对数据做清洗、转换、聚合的自动化任务，并编排各任务间的数据处理流程。

/ 变更数据捕获（CDC）：: 通过监控数据库日志或其他机制，捕捉到数据的每一次变化（增删改操作），然后将这些变化以结构化的方式提供给下游系统，如数据仓库、消息队列、流处理引擎等，实现数据的实时同步和分析。

/ 数仓分层：:	数仓分层是对数仓的表进行组织管理的技术维度，用于将不同用途的数据表，归类划分至不同的分层，便于更好地组织、管理、维护模型数据表。

/ 主题域：:	主题域是对贴源层和公共层数据表和视图进行组织和管理的维度，是根据业务分析对业务过程进行抽象的集合，一般一个主题域下的数据表均存在一定的关系

/ 应用域：:	应用域是面向应用场景或产品的数据组织，是对应用层的数据表进行组织和管理的维度，一般一个应用域下的数据表均存在一定的关系

/ 明细模型：:	明细模型（Duplicate）在某些多维分析场景下，数据既没有主键，也没有聚合需求，针对这种需求，可以使用明细数据模型

/ 唯一模型：:	当用户有数据更新需求时，可以选择使用唯一模型（Unique）。唯一模型能够保证 Key（主键）的唯一性，当用户更新一条数据时，新写入的数据会覆盖具有相同 key（主键）的旧数据

/ 聚合模型：:	聚合模型（Aggregate），当我们导入数据的时候, 会按照 key 对 value 使用他们自己的聚合类型进行聚合，经过聚合，Doris 中最终只会存储聚合后的数据。

/ 分区：:	分区用于将数据划分成不同区间，可以理解成把原始表划分成多个子表。可以方便的按分区对数据进行管理。分区可以视为是逻辑上最小的管理单元。

/ 分桶：:	分桶用于将数据划分为若干个数据分片，每个分桶包含若干数据行，各个分桶之间的数据相互独立。一个分桶只属于一个分区，但一个分区可以包含若干个分桶。分桶是数据移动、复制等操作的最小物理存储单元。


= 项目配置

在使用平台进行开发之前，首先需要对项目进行配置。

== 项目空间

#figure(
  rect(image("img/正式环境.png", width: 80%)),
  caption: [“正式环境”项目空间],
) <正式环境>

项目空间是任务的容器。不同项目空间的任务是不可见的。在集成开发平台中，如需创建数据同步、数据加工等任务，均需在已授权的项目空间下进行。用户可根据需要切换所在项目空间。*如无特殊情况，应使用“正式环境”项目空间*，如@正式环境 所示。项目空间主要有以下配置项：

+ 项目名称：项目的名称。

+ 描述：项目简介。

+ 主负责人：按需设置即可。

+ Worker资源组：选择“生产环境-worker集群”。可在页面#菜单([运维中心], [工作集群监控])中查看计算资源配置。

+ 开发模式：
  - 简单模式：开发和生产一体的项目开发模式，数仓引擎只对应生产环境一个数据源，数据加工任务的调试运行、调度运行都是操作生产环境的数据，创建的表和数据也只有一份。如果您追求极致的数据开发效率，且数据计算和存储资源有限，推荐此模式。

  - 标准模式（“正式环境”项目空间使用此模式）：开发和生产环境隔离的项目开发模式，数仓引擎可绑定开发和生产两个数据源，数据加工任务在调试运行时只操作开发环境的数据，在发布后调度运行时才操作生产环境数据。该模式下的库表和数据，通常需要在开发和生产环境各自创建一份，如果您对数据开发过程中的数据安全、流程规范有较高要求，且数据计算和存储资源充足，推荐此模式。

+ 权限模式：
  - 不允许他人修改：除项目负责人、平台管理员以及超级管理员外，同空间内自己创建的集成、开发任务只有自己能编辑、删除、上线、下线、提交发布、试运行及暂停。

  - 任务允许他人修改：同空间的集成、开发任务允许访客之外的其他成员修改，包括编辑、上线、下线、提交发布、试运行及暂停，但不包括删除（创建者、项目负责人、空间管理员和超级管理员可以删除）。

新建项目空间（@新建项目空间）需要具备高级权限，建议联系平台管理员创建并授权，或联系管理员先将用户授权为“平台管理员”角色后再自行创建项目空间。

#figure(
  rect(image("img/新建项目空间.png", width: 80%)),
  caption: [新建项目空间],
) <新建项目空间>

+ 进入#菜单([配置中心],[项目空间])页面，点击右上角新建项目空间按钮。

+ 选择对应项目空间名称，点击设置按钮，可将已有项目空间授权给自己或他人。如果在创建项目空间时已经指定了自己为主负责人，无需再执行授权操作。

== 成员角色（TODO）

== 数据源（TODO）

创建数据源连接。

进入配置中心->数据源页面，点击新建数据源按钮。根据上述操作准备的MySQL数据库环境，配置连接地址和账号等信息，数据源名称指定为“销售管理系统数据库-MySQL”。

注意，连接类型请选择MySQL，并选择实际的MySQL版本。由于是实时同步数据，该MySQL数据库需提前打开binlog相关设置，提供的数据库账号密码需具备采集binlog的权限（请参见数据集成主页面左下角数据库权限要求帮助文档中《CDC采集数据库配置》内的MySQL部分）。


= 外部数据同步

*应使用“数据集成”功能进行外部数据库的接入*。有“实时同步”和“定时同步”两种方式。如果是内部数据的同步或ETL操作，则应使用“数据开发”功能。

== 实时同步

进入数据集成页面，注意切换到正确的项目空间（“正式环境”），点击“新建同步任务”按钮，在弹框中选择实时同步后，点击确定按钮，即可新建实时同步任务。

“实时同步”将源端的数据变化实时地同步至目的端。此功能使用的是变更数据捕获（CDC）技术，数据变化的时延一般在秒级。注意现行版本中实时同步要求源数据表定义了主键。

尽管实时同步任务支持同时包含多个表（@实时同步多个表），但是为了方便监控和重跑等操作，仍然建议*一个表建一个任务，任务名称即目的表名称*。目的表名前缀可在“前缀设置”中指定（外部数据应落地在ODS层，故前缀一般为“`ODS_`”）。

#figure(
  rect(image("img/实时同步多个表.png", width: 80%)),
  caption: [实时同步多个表],
) <实时同步多个表>

*目的表结构不需要事先建好*。在任务设置中勾选“自动新建表”后将根据输入的表名以及所选字段在目的库自动创建表结构。当然如果已有目的表结构则不要勾选“自动新建表”。

一般来说应尽可能保留源表字段以备日后需要。但有些源表字段特别占用空间（如MySQL的`longtext`和`blob`等）且暂时没有明确的用处，也可不进行同步。实时同步默认同步所有字段的数据，也支持按需指定字段。进入实时同步任务的编辑页面，在“字段内容”栏内点击“编辑内容与策略”，然后在弹框中勾选需要同步数据的字段即可（@实时同步字段裁剪）。

#figure(
  rect(image("img/实时同步字段裁剪.png", width: 80%)),
  caption: [实时同步字段裁剪],
) <实时同步字段裁剪>

*字段一律勾选`etl_time`（由系统额外添加的输出信息：目的表数据写入/更新时间）*。这是因为后续数据加工过程可能会用到`etl_time`进行增量更新。

如果来源表的结构发生变化，如新增字段、修改字段类型、删除字段等，还可选择目的表结构是否跟踪这些变化。在实时同步任务的编辑页面，切换到高级配置标签页，按需勾选下方的3个选项（是否同步来源数据字段的新增、变化、和删除）即可。

配置完成后，点击右上角发布按钮。发布成功后，可在#菜单([运维中心], [实时集成任务])页面查看到该任务状态处于进行中，并采集到了数据。点击#菜单([异常日志])可查看运行报错信息。

=== 异常处理

#figure(
  rect(image("img/text_too_long.png", width: 80%)),
  caption: [the length of input string too long],
) <text_too_long>

同步`longtext`等长字符串时，可能偶尔会遇到`the length of input string too long`（@text_too_long）的错误。这是因为源字段的数据长度超过了目的表对应字段的限制。比如doris的`string`类型默认只支持最大1MB大小。可通过调整doris的`string_type_length_soft_limit`参数将`string`的最大长度调整为2GB（系统管理员操作）。

== 定时同步

“定时同步”也叫离线同步，它按照定制的调度计划，定时获取源端数据变化并同步至目的端。*调度的最小周期为一分钟*。不要求源数据表定义主键。

== 同步开发库与生产库

= 数据建模（TODO）

== 数仓规划

数据建模遵循“先设计，后开发”的原则，提供可视化的数仓规划和模型设计能力，将数据模型与企业架构保持一致，从源头上提高企业数据的一致性，帮助企业数据体系的规范化建设，将数仓规划落到实处，解决标准不统一、模型不一致的问题。

=== 数仓分层

*新建数仓分层*

a、进入数仓规划-分层设计页面，点击顶部【新建分层】或者在一级分层【公共层】下点击【新建自定义分层】，在弹窗页面填写分层名称、英文标识、所属分层、描述信息，选择数仓引擎环境和数据库，当模型设计完成发布物化至数据开发平台时，则模型将自动发布至分层所绑定的数据库，而无需每次发布时都选择数据库；

b、点击分层列表中新建的分层进入分层详情页面，在命名规则处点击【开始设置】，设置该分层下的数据表的前缀和后缀。

*如何将数仓分层与数据库进行绑定*

数据模型设计完成后进行物化发布时，物理表存储到哪个数据库中，则需要在分层设计时进行绑定。

平台提供贴源层、公共层和应用层3大一级分层，并提供ODS贴源层、DIM公共维度层、DWD明细数据层、DWS汇总数据层和ADS应用数据层5大系统默认常规分层，如果要使用系统提供的默认分层，则进入分层详情，为分层初始化绑定好数仓引擎和数据库，方法同新建数仓分层绑定数据库一样，可以选择只绑定开发或者生产一个环境，也可以都绑定，绑定之后，归属于该分层的模型发布时，则可以选择对应的环境并直接发布到绑定的数据库中。

如何对表命名进行规范设置

=== 主题域与应用域

*主题域与应用域的关系*

在模型设计和管理时，针对贴源层、公共层和应用层的数据表在业务组织和管理维度存在差异，例如贴源层、公共层的数据表通过数据域、业务过程进行抽象，应用层的数据表通常是对业务进行特定场景化的细分。
因此，主题域是针对贴源层和公共层数据表进行组织和管理，是对业务过程进行抽象的集合。主题域是按业务领域划分，一般是基于企业的业务架构进行组织。一个主题域下的数据表之间通常存在一定的关联关系。在模型设计时，当模型所属分层为贴源层和公共层的分层时，应选择主题域。
应用域是针对应用层的数据表进行组织和管理，是基于应用需求，按照数据集市的概念进行组织。一般一个应用域下的表，其数据是由多个主题域下的表进行汇总、打宽形成，满足多维度业务分析需要。在模型设计时，当模型所属分层为应用层的分层时，应选择应用域。 

*创建主题域/应用域*

本示例中的模型表为公共层模型，因此仅需要新建主题域，如果模型为应用层的模型，则需要创建应用域。创建主题域或应用域可以通过手动创建和导入2种方式，本示例则以手动创建为例进行操作。

a. 进入主题域页面，创建一级主题域：人资主题，并指定人资主题域负责人，主题域负责人仅能从模型设计师角色成员中选择；

b. 创建子主题域：员工域，子主题域负责人将自动继承上级负责人，也可以为子主题域再单独指定负责人；


== 模型设计

在进行数据建模前，平台管理员在配置中心对数据治理专员和模型设计师角色进行授权，只有数据治理专员能够进行数仓分层和主题域、应用域的设计，模型设计师能够进行模型设计。

*创建模型*

模型设计师进入模型设计页面，可以进行模型设计，模型设计支持手动创建、批量导入和逆向建模3种方式，本示例以手动创建方式为例进行操作。

a、 点击创建数据表，为模型选择所属分层和主题域、表类型，根据所选分层的表命名规范输入表名，责任人、责任部门、描述等模型属性；

b、 选择数据模型类型，新增模型字段；

c、 进入高级设置，进行分区、分桶和参数设置。

5) 发布模型

完成数据模型设计后，通常将模型提交发布至开发环境，开发环境测试无误后，再将模型提交至生产环境。

a、 在模型详情或者编辑页面点击【发布】，选择发布环境；

b、 选择模型发布模式，增量发布或者删除重建；

c、 点击【确定】即可完成模型发布，发布完成后可以即可以在模型设计所属主题域列表下查看已发布的模型。

*权限控制*

当企业业务比较复杂，需要由不同的模型设计师来管理自身业务的模型时，可以通过对主题域和应用域设置负责人的方式来对模型设计进行权限控制，即仅作为主题域/应用域的负责人时才能在该主题域/应用域及子主题域/应用域下创建模型，对于未负责的主题域模型则只能查看。

1） 进入主题域/应用域列表，通过新建或者编辑的方式进入主题域/应用域编辑页面，在域负责人处选择添加负责人；

2） 从配置中心模型设计师角色中已授权的人员中选择添加负责人；

3） 针对子主题域，除了可以为该层级主题域指定负责人外，系统将自动继承上级主题负责人作为该层级主题域的负责人，点击可查看所有上级继承的负责人列表，主题域负责人拥有该主题域及下属所有子主题域的模型创建权限。

*如何通过批量导入方式创建数据模型*

除了手动创建的方式创建数据模型外，您可以可通过导入的方式批量创建数据表。
前提条件：在模型导入前，需要先创建好数仓分层、主题域和应用域，并为主题域和应用域分配负责人权限，导入时也只能导入有权限的主题域或应用域下的模型；同时，不同分层的数据模型属性有差别，在导入前需要在数仓配置-模型属性设置中对不同分层的模型属性进行配置。

    1） 进入模型设计列表，在左侧目录中选择主题域或者应用域；

    2） 选择批量导入，下载模型模板；

    3） 在excel模板中根据模板填写说明，根据模型归属分层分别在对应的sheet页录入表模型信息和表字段信息；

    4） 上传文件，选择导入方式，包含跳过和更新2种方式，跳过即跳过系统中已存在的同名模型不做更新，更新即系统中存在的同名模型均以导入的信息为准。

    5） 导入完成后，系统将统计出本次导入成功和失败条数，并展示导入失败列表及失败原因，可以对导入失败的文件进行下载修改后重新导入。

*如何发布数据模型*

当逻辑模型创建完成后，您可以将模型发布至生产环境，调试无误后再发布至生产环境，当然，如果模型已经通过线下评审，则可以直接将模型发布至生产环境。

    1） 进入模型设计列表，可以通过列表上方模型状态筛选【未发布】的模型，选择需要发布的数据模型，点击编辑；

    2） 进入模型编辑页面，点击发布，选择生效环境和发布模型：
        a. 生效环境只能选择模型所属分层绑定了数据库的环境；
        b. 发布模型包含增量发布和删除重建，模型为首次发布时，无论选择增量发布还是删除重建都会在对应的环境中新建数据表。

    3） 点击确定即可完成发布，发布完成后，可以回到模型列表查看发布状态，如果发布异常可以查看异常日志。

*如何查看模型发布状态和发布记录*

数据模型发布后，将展示当前最新版本，但是系统会记录每一次发布历史，您可以查看模型发布记录。
    1） 进入模型设计列表页，可以查看所有模型的发布状态；

    2） 点击表名称，进入模型详情页面，可以查看模型当前版本；

    3） 点击发布记录，可以查看当前模型的所有发布记录，包含发布版本、环境、发布人、发布时间等。


*如何修改数据模型*

如果需要修改模型属性、添加或修改模型字段、修改分区分桶设置时，您可以对模型进行修改后再发布。

    1） 进入模型设计列表页，选择需要修改的模型，点击列表【编辑】进入模型编辑页面；

    2） 根据实际场景修改模型信息或者添加/修改字段、高级设置等信息；

    3） 点击【更新发布】，选择发布环境和发布方式，发布方式可选择增量发布或者删除重建。增量发布即在已有物理表上更新新增和修改的内容；删除重建即删除原有物理表后重新创建此次发布的模型。（注：当模型改变字段类型、改变字段长度、增加/删除key列、key列修改顺序、value列修改原有字段顺序、新增字段设置了非空、修改了高级配置（分区、分桶、参数任一项）则无法进行增量发布，只能选择删除重建。）

*如何删除数据模型的同时删除物理表*

支持对用于测试等废弃的模型进行删除，数据模型删除时，您可以选择只删除模型，也可同时删除物理表。

    1） 进入模型设计列表，选择需要删除的模型，点击列表【删除】进入删除确认页面；

    2） 进行二次确认要删除的模型信息，并勾选删除物理表；

    3） 点击确定，即可完成在删除模型的同时删除物理表。

*分区分桶设置*

为了能够高效处理大数据量的存储和计算，Doris提供分区和分桶2层数据划分方式来对数据进行分割处理，减少扫描成本，提高查询效率。

*动态分区设置*

动态分区旨在对表级别的分区实现生命周期管理 (TTL)，减少用户的使用负担。在某些使用场景下，用户会将表按照天进行分区划分，每天定时执行例行任务，这时通过动态分区功能，用户可以在建表时设定动态分区的规则。FE 会启动一个后台线程，根据用户指定的规则创建或删除分区，支持动态创建分区和动态删除分区。
在进行分区设置前，需要对数据量进行评估，当数据量大于1亿行时建议分区：
属性	说明
分区类型	默认不分区，可选择自定义分区，后续属性均为选择自定义分区之后的属性
分区字段	需选择一个date或datetime类型的字段，且该字段必须被设置为key列
时间粒度	动态分区创建的时间粒度，支持选择每小时、每天、每周和每月
分区保留策略	即动态删除最近的n个分区之前的数据
提前创建	支持预先创建分区，设置预先创建未来多少个分区
分区前缀	动态创建的分区名前缀

*分桶设置*

如果不使用分区，单独使用分桶设置是对整表进行数据划分。默认情况下，系统会按一定规则自动设置分桶列，用户无需关心，具体如下： 
唯一模式和聚合模式：默认以key列填充，支持用户设置多列，且必须从key列中选择； 

明细模式：默认key列，如不设置key列，则取前三列（第一列不能是float, double, string, array，jsonb，前三列中如果有float, double, string, array，jsonb类型，则截断），支持用户设置其他列（不限于Key列，不能是float, double, string, array，jsonb类型） 

使用分区设置时，分桶设置是对每个分区下的数据进行进一步划分。如按天分区，当每天的数据量很大时（超过500W），可以通过指定分桶列和分桶数量，合理划分不同分区的数据，分桶列建议选择区分度大的列，避免出现数据倾斜。

分桶建议： 
    a、 数据量大于500万行时建议分桶； 
    b、 分桶字段必须从key列选择； 
    c、 分桶列可以是多列，聚合模型和唯一模型必须为 Key 列，明细模型可以是 key 列和 value 列； 
    d、 分桶列可以和分区列相同或不同。
    

@员工表 展示了员工表的一些重要字段。

#let DWS_EMPLOYEE = csv("data/DWS_EMPLOYEE.csv")

#figure(
  table(
    columns: DWS_EMPLOYEE.first().len(),
    align: horizon + left,
    fill: (_, y) => if y == 0 {gray},
    ..DWS_EMPLOYEE.flatten()
  ), caption: [员工表（EMPLOYEE）]
) <员工表>


== 逆向建模

当您的物理引擎中已存在大量物理表，且希望通过数据建模来统一管理所有模型，则可以使用逆向建模来反向创建数据模型。

前提条件：已创建数仓分层和主题域或应用域，用于管理逆向的模型。

    1） 进入逆向建模菜单，创建逆向建模任务；

    2） 为需要逆向的模型选择所属分层；

    3） 选择逆向的模型执行方式：

        a. 全量覆盖：如系统中已存在同名模型，则以当前逆向的信息为准
        b. 增量更新：仅逆向系统中不存在的模型，已存在的模型则不逆向

    4） 选择需要逆向的物理表，您可以筛选环境选择生产环境或者开发环境的物理表进行逆向建模，同时您可以一次选择多张物理表；

    5） 完善模型信息；

    6） 选择开始逆向，即完成创建逆向任务，可以查看逆向任务详情，查看物理表逆向状态；

    7） 逆向完成后，即可进入模型设计列表，可以查看已逆向的模型信息。


= 数据开发(TODO)

创建数据加工任务，定时处理数据。
创建数据加工任务，定期执行数据处理，向目标表产出所需数据。
该示例中我们创建一个名为“销售订单数据加工-近实时”的数据流水线，在该流水线内，添加5个SQL类型的数据加工任务。
首先创建数据流水线。进入数据开发页面，在左侧选中数据流水线标签页


切换回到该数据流水线的编辑画布页面（可在顶部标签页中切换，或在左侧目录树中双击该流水线名称），继续拖动SQL类型的组件，直到5个SQL任务全部创建完成，在画布中出现5个任务节点。以下附5个SQL任务的SQL语句（可双击图标后使用文本编辑器打开）：

验证目标表的数据是否已产出。
进入数据开发页面，切换到数据表标签页下，鼠标移到目标表名称上，点击表详情图标，在弹框中切换到数据预览页面，查看该表已经有了数据。

```sql
insert into
    dw${env}.DWD_GYL_CGBJBIDDING --采购报价竞价表
    (
        id, --主键ID
        bjbm, --报价编码
        publish, --是否发布标志
        zdsj, --制单时间
        zdr, --制单人编码
        quotationdate, --报价日期
        ip, --竞价的IP地址
        etl_time --数据更新时间
    )
select
    id, --主键ID
    bjbm, --报价编码
    publish, --是否发布标志
    zdsj, --制单时间
    zdr, --制单人编码
    quotationdate, --报价日期
    ip, --竞价的IP地址
    etl_time --数据更新时间
from
    ods${env}.ODS_CG_CGBJBIDDING
where
    etl_time > timestampadd (minute, -2, now ());
```

*如何在SQL任务中快速编写SQL语句*

在数据开发的过程中，经常使用SQL类型的任务来处理数据，SQL代码中读写的数据表通常拥有数十上百的字段，人工编写代码耗时费力、容易出错。可使用快捷功能加速SQL代码编写过程。

在SQL任务的编辑页面，点击左侧数据表标签页，定位到SQL代码中需要读写的数据表；鼠标移到表名称上方后出现功能图标，点击表详情图标后弹出字段信息页面，选中所需字段后点击复制select语句或复制insert语句，即可快速复制包含字段和表的SQL语句，可直接粘贴到编辑中的SQL任务中使用。

*如何让数据加工任务在数据同步完成后及时运行*

在对平台中的数据做加工时，数据加工任务在什么时间点触发运行很重要，通常有两点诉求：一是在任务运行时需要确保待处理的来源数据已经准备就绪，而是希望任务触发运行的时间越早越好（保障数据及时产出）。

如果数据来源为实时同步的数据，下游数据加工任务只需按照业务需求，设置定时调度即可，因为实时同步的数据延迟较低（秒级别），任务运行时读取的数据已经是最新完整的数据。

如果数据来源为离线同步的数据，则建议将数据加工任务和离线数据同步任务建立上下游依赖关系，当离线同步任务完成后触发加工任务运行。该场景下建议使用数据开发中的数据离线同步组件完成数据同步，以便可跟下游加工任务构建依赖调度关系，而不是数据集成页面中的定时同步功能。

进入数据开发页面，打开对应数据流水线的画布编辑页面，拖动创建类型为数据离线同步的组件，完成任务配置后，将其与下游数据加工任务连线，形成上下游依赖关系。


*使用变量来实现任务运行时的动态传参*

对于周期调度执行的数据加工任务，通常需要动态传参来运行，常见的例如每次运行时传入日期参数来过滤数据。

系统内置支持常见的日期变量，在编辑SQL等类型任务时，可在工具栏中点击变量说明进行查看。

例如以下SQL任务中，在SQL代码中指定了日期变量\${system.biz.date}，任务将在每次运行时自动替换变量的值为实例运行时的前一天，格式为yyyyMMdd，例如20240505。

*数据探查功能中为什么有些SQL语句不能执行*

数据探查功能，仅为了方便用户查询生产环境的数据，但为了保护生产环境的数据不被随意修改，平台禁用了drop、alter、delete等修改数据库表结构或数据的SQL语句，目前仅支持select、desc、show、explain等查询类SQL语句。

= 运维监控

在“运维中心”可以监控各任务运行的情况、浏览任务报错日志、启动、重跑、停止任务等。此外，还可以监控整个“集成开发平台”产品运行节点的健康状况。

== 实时集成任务

进入#菜单([运维中心],[实时集成任务])页面，在任务运维标签页的列表中找到相关任务，然后在右侧的操作栏中点击#菜单([数据统计])，即可查看历史同步的数据量和同步速度（@实时集成数据统计）等健康参数。

#figure(
  rect(image("img/实时集成数据统计.png", width: 80%)),
  caption: [实时集成数据统计],
) <实时集成数据统计>

*对实时集成任务，应重点关注异常日志*（#菜单([运维中心],[实时集成任务], [异常日志])）。如果报错是暂时性的（如@实时集成日志），则可以不必理会，因为CDC机制会在后续的同步成功时弥补以前的缺失。

#figure(
  rect(image("img/实时集成日志.png", width: 80%)),
  caption: [实时集成日志],
) <实时集成日志>

当源端业务系统的数据或数据结构发生重大变化、或数据由前期试用阶段转为正式投产等特殊情况时，可能想要*清空数据平台中已经实时同步的数据，并重新同步全量数据*。进入#菜单([运维中心],[实时集成任务])页面，在任务运维标签页的列表中找到相关任务，然后在右侧的操作栏中点击#菜单([重跑])（@重跑实时集成任务）。该操作可将该任务内同步数据的目的表进行清空，然后重新从来源表同步全量数据到目的表中。

#figure(
  rect(image("img/重跑实时集成任务.png", width: 80%)),
  caption: [重跑实时集成任务],
) <重跑实时集成任务>

== 离线集成和数据加工任务

*对离线集成和数据加工任务，应关注最近一个时段的失败任务*。用筛选条件查询（如@数据加工失败任务查询），再查看失败任务的日志。

#figure(
  rect(image("img/数据加工失败任务查询.png", width: 80%)),
  caption: [数据加工失败任务查询],
) <数据加工失败任务查询>

注意：如果任务频率较高（如每分钟都运行），则系统会积累下大量的运行记录和日志，造成后续查询缓慢甚至无法查询。目前的解决方案是只查最近时段（如最近一小时）且状态为“失败”的任务。

== 集群监控

#figure(
  rect(image("img/集群监控.png", width: 80%)),
  caption: [集群监控],
) <集群监控>

集群监控的页面如@集群监控 所示。 需要注意的是，无论是调度集群还是工作集群，*监控的对象都是产品功能的运行节点，而不是业务数据的存储节点*。也就是说，@集群监控 中那几百G的“磁盘可用空间”不是 doris 数据存储的磁盘空间。这些节点执行调度任务、转发任务，以及 python、shell 这些任务也都会在上面运行。占用磁盘空间的主要是系统日志文件等。

当任务监控界面出现报错或无法查询等异常时，不妨浏览一下集群监控界面，看有没有异常（高CPU使用率或内存占用、磁盘可用量枯竭等），或许可以为定位问题提供一些提示。


